\documentclass{sig-alternate-05-2015}
\usepackage{hyperref}

\begin{document}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

\title{Predicting Video Tags Using Google's YouTube-8M Dataset (tentative title)}

\numberofauthors{1} 
\author{
% 1st. author
% \alignauthor
Tatyana Li \\
	\email{litatyan@msu.edu}
}
\date{17 February 2017}

\maketitle

\section{Problem Description}
On September 2016, Google announced a release of the large labeled dataset, YouTube-8M, for video understanding research. The dataset contains about 8 million YouTube videos, amounting to more than 500K hours of video content. Earlier this month, \textbf{Google hosted a Google Cloud and YouTube-8M Video Understanding Challenge} on a Kaggle platform, providing an updated version of the dataset with new labels and newly added audio features. 
This project will use the above dataset to implement and compare classification algorithms to predict video labels based on the video and frame-level features provided.

\section{Related Work}
[Note: Due to the fact that Google announced the above competition two days ago, the survey of the related work presented below is limited, and will be expanded further in future reports.] 

There has been significant progress in the area of video understanding. YFCC-100M datasets \cite{1} with 800K videos and metadata, containing video titles, descriptions and tags; ActivityNet \cite{2}, which is a large-scale video benchmark with several thousand videos and 200 human activity classes, and can be used to compare algorithms for human activity understanding. As of today, the Sports-1M \cite{3} has been considered one of the largest video datasets, containing around 1M video instances. 
However, there has been no video datasets, comparable in scale and diversity to YouTube-8M. The updated YouTube-8M dataset contains over 8M videos and 1.9B video frames and provides the richest resource for research in video understanding and representation learning.

\section{Preliminary Plan}
\textbf{What will be done between now and intermediate report due date (March 17, 2017).} \\
Due to the large size of the data files (31 GB of video-level data, about 1.7 TB of frame-level data), they are hosted on Google Cloud, along with the training and validation sets ground truth labels. I will set up a Google Cloud account to retrieve the training and test files, as well as set up the required environment using cloud shell by installing pre-requisite packages and dependencies and the latest version of TensorFlow. Next, I will work on a more comprehensive survey of the existing research related to video understanding and the machine learning algorithms used to predict the key labels of a video. Along with literature review, I will work on exploring the data and finalize data-preprocessing and feature engineering. Along with the release of the dataset, Google Research team also released a technical report\cite{4}, where the authors used Support Vector Machine, Logistic Regression and Mixture of Experts to classify video labels. The classifiers reported by the authors could be a good set of models to start with and the reported classification results will be used as benchmark baseline results. Ultimately, the purpose of this project is to identify the classification model that gives the best label predictions based on the test data provided by Google, which is not publicly available.

\textbf{What will be done between March 17, 2017 and Final Report due date (April 28, 2017).} \\
Within this timeframe, I will work on implementation of the classification algorithms, with intermediate submissions of the predictions on Kaggle. That way, I would be able to evaluate the prediction accucacy of the model on the test set, which is not publicly available. At this time, all other required sections of the report would also be expanded, refined and completed by the project due date. 

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.

% \bibliographystyle{abbrv}
% \bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case

% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

% \section{References}
% Generated by bibtex from your ~.bib file.  Run latex,
% then bibtex, then latex twice (to resolve references)
% to create the ~.bbl file.  Insert that ~.bbl file into
% the .tex source file and comment out
% the command \texttt{{\char'134}thebibliography}.


\bibliography{YouTube-8M}
\bibliographystyle{unsrt}
\end{document}
